{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b58d3ff1-c952-4cd2-987a-85e6033e8597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai import APIConnectionError\n",
    "from openai import RateLimitError\n",
    "from openai import APIStatusError\n",
    "from tenacity import retry, wait\n",
    "\n",
    "import replicate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b5b7ccbe-b145-4204-81c4-1b7e2abc708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing MMLU parquet files\n",
    "class_name = \"all\"\n",
    "types = [\"auxiliary_train\", \"dev\", \"test\", \"validation\"]\n",
    "dfa = []\n",
    "\n",
    "for type_name in types:\n",
    "    dfa.append(pd.read_parquet(\"mmlu-test/{}/{}-00000-of-00001.parquet\".format(class_name, type_name), engine=\"fastparquet\"))\n",
    "df = pd.concat([dff for dff in dfa], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "af680874-6e43-4e01-b74f-88e0fc351c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "default_system_message = \"\"\"What is the color of the sky? Answer in one word\"\"\"\n",
    "\n",
    "# seed_model_system_message = \"\"\"You are an evaluator agent. You have access to a large dataset of multiple choice questions \\\n",
    "                        # from the MMLU dataset but you won't interact with the dataset directly. Instead you will generate \\\n",
    "                        # a random number between 0 to 1000. The question at this number will be presented to a test LLM and its response \\\n",
    "                        # given to you. The test model will be presented with the question and a list of choices. The test model is supposed \\\n",
    "                        # to return an index. You have to check that the index is valid that is, corresponds to a choice in the list. Moreover,\\\n",
    "                        # you have to check if the LLM correctly answered the question by comparing with the correct answer index. Based \\\n",
    "                        # on the result, you will either output the string is_correct or is_wrong\"\"\"\n",
    "\n",
    "seed_model_system_message = \"\"\"\n",
    "You have access to a number of functions which you can call by generating a json formatted python string where \\\n",
    "the fn field is a mapping from integer to the function to be called. Rest of the fields are paramters to the function if any. \\\n",
    "You can generate a random number between start and end values by setting fn to 0 and setting appropriate values for start and end. \\\n",
    "You can also sample from a dataset by setting fn to 1. This function does not require any arguments. \\\n",
    "You must only give the command, no need to explain. Strictly only the json formatted string!\n",
    "\"\"\"\n",
    "\n",
    "test_model_system_message = \"\"\"\n",
    "You will be asked a multiple choice question. You have to output the correct option number and nothing else. Just one valid integer. \\\n",
    "The format of the question will be as follows:\n",
    "Question: <QUESTION>\n",
    "Choices: 1. <CHOICE 1> 2. <CHOICE 2> and so on. \n",
    "Make sure that you ONLY output one valid integer from 1 to the last choice index for the question!\n",
    "\"\"\"\n",
    "\n",
    "oai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "247cb2e7-b4da-47aa-9466-14dd2a5d6ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"fn\": 0, \"start\": 0, \"end\": 1000}'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(replicate.run(\"meta/meta-llama-3-8b-instruct\", input={\n",
    "    \"prompt\": \"Call the appropriate function to generate a number between 0 and 1000\",\n",
    "    \"system_prompt\": seed_model_system_message\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e2d0c1d7-bb82-4dde-a9ff-c91feccfb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent\n",
      "{'n_questions': 5, 'n_correct': 3, 'n_wrong': 2, 'n_errors': 0, 'accuracy': 0.6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A custom agent to evaluate a given TEST_MODEL using randomly selected MCQ questions from the MMLU dataset\n",
    "# The SEED_MODEL is a very basic LLM which orchestrates the evaluation process\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, dataset, seed_model=\"meta/meta-llama-3-8b-instuct\", test_model=\"gpt-3.5-turbo\", temperature=1):\n",
    "        print(\"Starting agent\")\n",
    "        self.history = []\n",
    "        self.temperature = temperature\n",
    "        self.dataset = dataset\n",
    "        self.seed = seed_model\n",
    "        self.test_model = test_model\n",
    "\n",
    "    def set_test_model(test_model):\n",
    "        self.test_model = test_model\n",
    "\n",
    "    def get_test_model():\n",
    "        return self.test_model\n",
    "    \n",
    "    def get_response(self, question, choices):\n",
    "        try:\n",
    "            choice_str = \" \".join([ str(idx+1) + \". \" + choice + \" \" for idx, choice in enumerate(choices)])\n",
    "            # print(question + \" \" + choice_str)\n",
    "            question_str = question + \" \" + choice_str + \" What is the correct answer?\"\n",
    "            self.history.append({\"role\": \"system\", \"content\": question_str })\n",
    "            \n",
    "            if self.test_model == \"gpt-3.5-turbo\":\n",
    "                response = oai.chat.completions.create(\n",
    "                    model=self.test_model, messages=self.history, temperature=self.temperature\n",
    "                )\n",
    "                return {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "            elif self.test_model == \"meta/meta-llama-3-8b-instruct\": \n",
    "                return \"\".join(replicate.run(\"meta/meta-llama-3-8b-instruct\", input={\n",
    "                    \"prompt\": question_str,\n",
    "                    \"system_prompt\": test_model_system_message\n",
    "                }))\n",
    "        except APIConnectionError as e:\n",
    "            print(\"The server could not be reached\")\n",
    "            print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "        except RateLimitError as e:\n",
    "            print(\"A 429 status code was received; we should back off a bit.\")\n",
    "        except APIStatusError as e:\n",
    "            print(\"Another non-200-range status code was received\")\n",
    "            print(e.status_code)\n",
    "            print(e.response)\n",
    "\n",
    "    def _gen_random(self, start, end):\n",
    "        return random.randint(start, end)\n",
    "\n",
    "    def _get_question_from_mmlu(self, index):\n",
    "        return self.dataset.loc[index]\n",
    "\n",
    "    def get_question_set(self):\n",
    "        r = self.execute_fncall(\n",
    "            \"\".join(replicate.run(\n",
    "             \"meta/meta-llama-3-8b-instruct\",\n",
    "                input={\n",
    "                \"prompt\": \"Call the appropriate helper function to sample a random question from the dataset\",\n",
    "    \"system_prompt\": seed_model_system_message,\n",
    "                })))\n",
    "        return r\n",
    "        \n",
    "    def evaluate_model(self, n_questions):\n",
    "        n_correct = 0\n",
    "        n_wrong = 0\n",
    "        n_errors = 0\n",
    "        for i in range(n_questions):\n",
    "            try:\n",
    "                question_set = self.get_question_set()\n",
    "                question = question_set['question']\n",
    "                choices = question_set['choices']\n",
    "                response = self.get_response(question, choices)\n",
    "                \n",
    "                if int(response[0]) == (question_set['answer'] + 1):\n",
    "                    n_correct += 1\n",
    "                else:\n",
    "                    n_wrong += 1\n",
    "            except TypeError as te:\n",
    "                print(\"Expected integer response from test model, got something else\", te)\n",
    "                n_errors += 1\n",
    "            except:\n",
    "                raise\n",
    "                print(\"Something unexpected occurred\")\n",
    "                n_errors += 1\n",
    "        return { \"n_questions\": n_questions, \"n_correct\": n_correct, \"n_wrong\": n_wrong, \"n_errors\": n_errors\n",
    "            , \"accuracy\": n_correct / (n_questions - n_errors) }\n",
    "        \n",
    "    def execute_fncall(self, response):\n",
    "        try:\n",
    "            res = json.loads(response)\n",
    "            if res['fn'] == 0:\n",
    "                # call random number \n",
    "                return self._gen_random(res['start'], res['end'])\n",
    "            elif res['fn'] == 1:\n",
    "                # call get question\n",
    "                return self._get_question_from_mmlu(self._gen_random(0, self.dataset.shape[0]))\n",
    "            else:\n",
    "                raise ValueError(\"Non existent function called {}\".format(res.fn))\n",
    "        except ValueError as e:\n",
    "            print(\"Non existent function called\", e)\n",
    "        except:\n",
    "            print(\"Some unknown error occured\")\n",
    "            raise # handle later by logging to error file\n",
    "\n",
    "def main():\n",
    "    agent = Agent(df, test_model=\"meta/meta-llama-3-8b-instruct\") # temperature\n",
    "    # print(agent.get_question())\n",
    "    # print(agent.get_response(\"What is the color of the sky?\", [\"Red\", \"Orange\", \"Green\", \"Blue\"]))\n",
    "    print(agent.evaluate_model(5))\n",
    "    \n",
    "    # print(agent.execute_fncall(\"{\\\"fn\\\": 1 }\"))\n",
    "    # d = { \"fn\": 0, \"start\": 0, \"end\": 1000 }\n",
    "    # print(json.dumps(d))\n",
    "    # print(df.shape) # ignoring the class field as questions are mixed, this param can be used to find perf on various class of questions\n",
    "    # print(agent.get_response())\n",
    "    # print(agent.get_question_from_mmlu(agent.gen_random(0, 115700)).iloc[0])\n",
    "    # test_model1 = TestModel(model_id=\"gpt-3.5-turbo\")\n",
    "    # agent.set_test_model(test_model1)\n",
    "    # test_model_ids = agent.get_test_models() # dict { index: model_name }\n",
    "    # agent.evaluate_model([0]) # return { n_questions, n_correct, n_wrong, n_errors }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
